
## TODO:
## - Handling 1rpm peak cut off
## - Handling downstream pruning for zero peaks sample


########################
## Basic parameters

## Sample information file with five columns: id / name / group / fq1 / fq2
src_sampleInfo	= "sample.tsv"
cluster_yml		= os.environ["CHIP_PATH"] + "/Snakemake.ChIP_SE/cluster.yml"

## Genome folder depending on platform: CCHMC:HPC vs my desktop
#import socket
#hostname = socket.gethostname()	

# mm10
genome		= "mm10"
genomeFa	= "/data/limlab/Resource/GenomeData/mm/mm10/Genome/genome.fa"
chrom_size	= "/data/limlab/Resource/GenomeData/mm/mm10/Genome/chrom.size"
peak_mask	= "/data/limlab/Resource/GenomeData/mm/mm10/ENCODE-blacklist.bed"
star_index	= "/data/limlab/Resource/STAR_Index/2.7.4/mm10_allChr_vM20"

'''
## hg38
genome		= "hg38"
genomeFa	= "/data/limlab/Resource/GenomeData/hg/hg38/Genome/genome.fa"
chrom_size	= "/data/limlab/Resource/GenomeData/hg/hg38/Genome/chrom.size"
peak_mask	= "/data/limlab/Resource/GenomeData/hg/hg38/ENCODE-blacklist.bed"
star_index	= "/data/limlab/Resource/STAR_Index/2.7.4/hg38_allChr_v29"
'''


## Other essential pameters
adapter			= "AGATCGGAAGAGC"	# illumina universal adapter
#adapter			= "CTGTCTCTTATA"	# Nextera adapter for ATAC-seq or Cut&Tag

#trim_maxLen=100	## Maximum read length after trimming. ** NOT YET IMPLEMENTED **
trim_minLen		= 20
trim_minQual	= 20
chrRegexAll		= "^chr[0-9XY]+$"
chrRegexTarget	= "^chr[0-9XY]+$"
spikePrefix		= "NULL"

########################
## STAR options
star_module	= "STAR/2.7.4"

## ChIP-seq for SE
star_option	= "--alignSJDBoverhangMin 999 --alignIntronMax 1 --outFilterMultimapNmax 1 --outFilterMismatchNoverLmax 0.05 --outReadsUnmapped None"
## Note:
## Additoinal star_option to prevent soft-clipping: "--alignEndsType EndToEnd"
## BAM sort by "--outSAMtype" is handled by the star.align.sh by -s option.
## To keep unmapped reads in the output bam file, add "--outSAMunmapped Within"

#########################
## Job flags
## Adapter trimming
doTrim		= False
## Deduplication of BAM file
doDedup		= False
## Robust fragment length estimation in Homer tag directory
## Sometimes, Homer's default fragment length estimation is incorrect
## If robustFragLen = True, fragment length is estimation is performed separately and override Homer result
robustFragLen = False

##########################
## Down sampling parameter
## - Down sampling is done usings BAM file
## NOTE: Currently it is done in read level not fragment level for paired end sequencing. so may result in dangling read i.e. without the other read in pair.
## NOTE: In the current version, bam file is automatically sorted by coordinate upon alignment
## PLAN: Update is planned to incorporate option to handle fragment-level downsampling.
## - Down sampling depth can be defined in two ways
##   1. downSampleN variable in Snakefile: equally applied to all samples. If 0, no down sampling.
##   2. "DownSample" column in sample.tsv file: sample by sample down sampling depth.
##      If sample.tsv has "DownSample" column, this override "downSampleN"
##      Samples with DownSample > 0 will be down sampled accordingly.
##   If doDedup == True, deduplication is done *after* down-sampling
downSampleN	= 0

#########################
## Directories
fastqDir	= "0.Fastq"
trimDir		= "0.Fastq.Trim"
alignDir	= "1.1.Align"
downsampleDir	=	"1.2.Align.downSample"
dedupDir	=	"1.3.Align.dedup"
sampleDir	= "2.Samples"

bigWigDir_avg="3.1.bigWig_avg"

########################
## Other parameteres
## Homer "tbp" option, number of read per position to consider
## If 0, all the reads are considered.
## If deduplication is done, 0 is strongly recommended.
## Applied only when making tagDirectory.
## Peak calling uses all reads (tbp=0) in the tag directory
Homer_tbp	= 1	# Homer tbp

################################
## Loading sample Information
import pandas as pd
import sys
samples = pd.read_csv(src_sampleInfo, sep="\t", comment="#", na_filter=False)
if not samples.Id.is_unique:
	print( "Error: Id column in sample.tsv is not unique")
	sys.exit()
if not samples.Name.is_unique:
	print( "Error: Name column in sample.tsv is not unique")
	sys.exit()

#################################
## Cluster configuration file
#cluster = json.load(open("./cluster.json"))
import yaml
with open(os.path.expanduser(cluster_yml), 'r') as fh:
	cluster = yaml.load(fh)
#    cluster = yaml.full_load(fh)





#########################
## Rules start
#def getGroupToPool():
#	# groups that has more than one replicates
#	groupWithRep = set(samples.Group.value_counts().index[ samples.Group.value_counts() > 1 ])
#	# groups that is not input/control
#	groupNoneCtrl = set(samples.Group[samples.Ctrl != "NULL"])
#	groups = list( groupWithRep & groupNoneCtrl )
#	return groups
samplesAll	= samples.Name.tolist()
samplesFactor	= samples.Name[ samples.PeakMode=="factor" ].tolist()
samplesHistone	= samples.Name[ samples.PeakMode=="histone" ].tolist()
groupsAll		= samples.Group.unique().tolist()
groupsChIP		= samples.Group[ ( samples.PeakMode == "factor" ) | ( samples.PeakMode == "histone" ) ].unique().tolist()

rule all:
	input:
		expand(sampleDir + "/{sampleName}/HomerPeak.factor/peak.exBL.1rpm.bed", sampleName=samplesFactor),
		expand(sampleDir + "/{sampleName}/HomerPeak.histone/peak.exBL.bed", sampleName=samplesHistone),
		expand(sampleDir + "/{sampleName}/Motif/Homer.all/homerResults.html", sampleName=samplesFactor),
		expand(sampleDir + "/{sampleName}/Motif/MEME.random5k/meme-chip.html", sampleName=samplesFactor),
		expand(sampleDir + "/{sampleName}/igv.bw", sampleName=samplesAll)
		#expand(bigWigDir_avg + "/{groupName}.avg.bw" groupName=groupsAll),
		#expand(bigWigDir_avg + "/{groupName}.avg.subInput.bw" groupName=groupsChIP)

include: os.environ["CHIP_PATH"] + "/Snakemake.ChIP_SE/rules.pre.smk"
include: os.environ["CHIP_PATH"] + "/Snakemake.ChIP_SE/rules.post.smk"
